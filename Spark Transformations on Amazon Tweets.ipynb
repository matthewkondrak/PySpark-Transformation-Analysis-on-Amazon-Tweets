{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Spark RDD Operations (Transformation and Actions) on 400K Amazon Tweets - Matthew Kondrak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql.functions as sq \n",
    "from pyspark.sql.functions import concat,col,lit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 16:57:46 WARN Utils: Your hostname, Matthews-MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 192.168.68.57 instead (on interface en0)\n",
      "22/05/27 16:57:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/27 16:57:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/27 16:57:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('RDD').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.57:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10812d0a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the Amazon Tweets Dataset: (413247, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_pyspark = spark.read.option('header','true').csv('/Users/matthewkondrak/Desktop/Amazon_Tweets.csv', inferSchema=True)\n",
    "\n",
    "#size of data\n",
    "print(\"Size of the Amazon Tweets Dataset:\",(data_pyspark.count(), len(data_pyspark.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_str',\n",
       " 'tweet_created_at',\n",
       " 'user_screen_name',\n",
       " 'user_id_str',\n",
       " 'user_statuses_count',\n",
       " 'user_favourites_count',\n",
       " 'user_protected',\n",
       " 'user_listed_count',\n",
       " 'user_following',\n",
       " 'user_description',\n",
       " 'user_location',\n",
       " 'user_verified',\n",
       " 'user_followers_count',\n",
       " 'user_friends_count',\n",
       " 'user_created_at',\n",
       " 'tweet_language',\n",
       " 'text_',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only important columns for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "|              id_str|    tweet_created_at|user_verified|favorite_count|retweet_count|               text_|\n",
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "|'793270689780203520'|Tue Nov 01 01:57:...|        False|             0|            0|@AmazonHelp Can y...|\n",
      "|'793281386912354304'|Tue Nov 01 02:39:...|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793501578766319616'|Tue Nov 01 17:14:...|        False|             0|            0|@AmazonHelp It wa...|\n",
      "|'793501657346682880'|Tue Nov 01 17:15:...|        False|             0|            0|@AmazonHelp I am ...|\n",
      "|'793502854459879424'|Tue Nov 01 17:19:...|         True|             0|            0|@SeanEPanjab Plea...|\n",
      "|'793504235400884224'|Tue Nov 01 17:25:...|         True|             0|            0|@SeanEPanjab With...|\n",
      "|'793511847899070465'|Tue Nov 01 17:55:...|        False|             0|            0|@AmazonHelp It wa...|\n",
      "|'793511899279208449'|Tue Nov 01 17:55:...|        False|             0|            0|@AmazonHelp if it...|\n",
      "|'793513446633533440'|Tue Nov 01 18:02:...|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793299404975247360'|Tue Nov 01 03:51:...|        False|             0|            0|@JeffBezos @amazo...|\n",
      "|'793301295255945216'|Tue Nov 01 03:59:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793407430344310785'|Tue Nov 01 11:00:...|        False|             0|            0|@AmazonHelp How m...|\n",
      "|'793423313674571776'|Tue Nov 01 12:03:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793423314333134850'|Tue Nov 01 12:03:...|         True|             0|            0|@aakashwangnoo Pl...|\n",
      "|'793467086869630977'|Tue Nov 01 14:57:...|        False|             0|            0|@AmazonHelp @amaz...|\n",
      "|'793492430666498050'|Tue Nov 01 16:38:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793535036213501952'|Tue Nov 01 19:27:...|        False|             0|            0|@AmazonHelp @amaz...|\n",
      "|'793535221329113088'|Tue Nov 01 19:28:...|        False|             0|            0|@AmazonHelp @amaz...|\n",
      "|'793537840533471232'|Tue Nov 01 19:38:...|        False|             0|            0|@AmazonHelp @amaz...|\n",
      "|'793538125884645376'|Tue Nov 01 19:40:...|        False|             0|            0|@AmazonHelp @amaz...|\n",
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pyspark1=data_pyspark.select('id_str', 'tweet_created_at', 'user_verified', 'favorite_count', 'retweet_count', 'text_')\n",
    "data_pyspark1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal 1:\n",
    "\n",
    "Filtering user_verified to only show TRUE values\n",
    "\n",
    "For the verified users, grouping by created date, and counting the number of tweets for each date\n",
    "\n",
    "With the date with highest number of tweets, calculating the sum of favorite_count and retweet_count for each tweet on that day.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "|              id_str|    tweet_created_at|user_verified|favorite_count|retweet_count|               text_|\n",
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "|'793281386912354304'|Tue Nov 01 02:39:...|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793502854459879424'|Tue Nov 01 17:19:...|         True|             0|            0|@SeanEPanjab Plea...|\n",
      "|'793504235400884224'|Tue Nov 01 17:25:...|         True|             0|            0|@SeanEPanjab With...|\n",
      "|'793513446633533440'|Tue Nov 01 18:02:...|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793301295255945216'|Tue Nov 01 03:59:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793423313674571776'|Tue Nov 01 12:03:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793423314333134850'|Tue Nov 01 12:03:...|         True|             0|            0|@aakashwangnoo Pl...|\n",
      "|'793492430666498050'|Tue Nov 01 16:38:...|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793551822476705793'|Tue Nov 01 20:34:...|         True|             0|            0|@aakashwangnoo He...|\n",
      "|'793551823147765765'|Tue Nov 01 20:34:...|         True|             0|            0|@aakashwangnoo at...|\n",
      "|'793676510494359552'|Wed Nov 02 04:50:...|         True|             0|            0|@aakashwangnoo So...|\n",
      "|'793322306848292864'|Tue Nov 01 05:22:...|         True|             0|            0|@Murtazansp Hello...|\n",
      "|'793322433625415680'|Tue Nov 01 05:23:...|         True|             0|            0|@Murtazansp Could...|\n",
      "|'793872301339148288'|Wed Nov 02 17:48:...|         True|             0|            0|@Murtazansp Hi, s...|\n",
      "|'793872423271661568'|Wed Nov 02 17:48:...|         True|             0|            0|@Murtazansp Pleas...|\n",
      "|'795207462160896001'|Sun Nov 06 10:13:...|         True|             0|            0|@Murtazansp Hey t...|\n",
      "|'793375905280393216'|Tue Nov 01 08:55:...|         True|             0|            0|@anchitkhar Hello...|\n",
      "|'793397280254472193'|Tue Nov 01 10:20:...|         True|             0|            0|@anchitkhar HI! W...|\n",
      "|'793534958073679872'|Tue Nov 01 19:27:...|         True|             0|            0|@anchitkhar Hey! ...|\n",
      "|'793378044052406272'|Tue Nov 01 09:04:...|         True|             0|            0|@imkapsicum Hey, ...|\n",
      "+--------------------+--------------------+-------------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only showing verified users\n",
    "data_pyspark1 = data_pyspark1.filter(data_pyspark1.user_verified == 'True')\n",
    "data_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets by Verified Users: 171797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#counting the Number of Tweets by Verified Users\n",
    "print(\"Number of Tweets by Verified Users:\", data_pyspark1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
      "|              id_str|tweet_created_at|user_verified|favorite_count|retweet_count|               text_|\n",
      "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
      "|'793281386912354304'|          Nov 01|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793502854459879424'|          Nov 01|         True|             0|            0|@SeanEPanjab Plea...|\n",
      "|'793504235400884224'|          Nov 01|         True|             0|            0|@SeanEPanjab With...|\n",
      "|'793513446633533440'|          Nov 01|         True|             0|            0|@SeanEPanjab I'm ...|\n",
      "|'793301295255945216'|          Nov 01|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793423313674571776'|          Nov 01|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793423314333134850'|          Nov 01|         True|             0|            0|@aakashwangnoo Pl...|\n",
      "|'793492430666498050'|          Nov 01|         True|             0|            0|@aakashwangnoo Hi...|\n",
      "|'793551822476705793'|          Nov 01|         True|             0|            0|@aakashwangnoo He...|\n",
      "|'793551823147765765'|          Nov 01|         True|             0|            0|@aakashwangnoo at...|\n",
      "|'793676510494359552'|          Nov 02|         True|             0|            0|@aakashwangnoo So...|\n",
      "|'793322306848292864'|          Nov 01|         True|             0|            0|@Murtazansp Hello...|\n",
      "|'793322433625415680'|          Nov 01|         True|             0|            0|@Murtazansp Could...|\n",
      "|'793872301339148288'|          Nov 02|         True|             0|            0|@Murtazansp Hi, s...|\n",
      "|'793872423271661568'|          Nov 02|         True|             0|            0|@Murtazansp Pleas...|\n",
      "|'795207462160896001'|          Nov 06|         True|             0|            0|@Murtazansp Hey t...|\n",
      "|'793375905280393216'|          Nov 01|         True|             0|            0|@anchitkhar Hello...|\n",
      "|'793397280254472193'|          Nov 01|         True|             0|            0|@anchitkhar HI! W...|\n",
      "|'793534958073679872'|          Nov 01|         True|             0|            0|@anchitkhar Hey! ...|\n",
      "|'793378044052406272'|          Nov 01|         True|             0|            0|@imkapsicum Hey, ...|\n",
      "+--------------------+----------------+-------------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fixing the tweet_created_at format to a more usable format for grouping\n",
    "a = split(data_pyspark1['tweet_created_at'], ' ')\n",
    "data_pyspark2 = data_pyspark1.withColumn('Month', a.getItem(1))\n",
    "data_pyspark2 = data_pyspark2.withColumn('Date', a.getItem(2))\n",
    "\n",
    "data_pyspark3 = data_pyspark2.withColumn('tweet_created_at', sq.concat(sq.col('Month'), sq.lit(' '), sq.col('Date')))\n",
    "data_pyspark3 = data_pyspark3.select('id_str', 'tweet_created_at', 'user_verified', data_pyspark3.favorite_count.cast('int'), data_pyspark3.retweet_count.cast('int'), 'text_')\n",
    "data_pyspark3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|tweet_created_at|tweet_count|\n",
      "+----------------+-----------+\n",
      "|          Nov 27|        208|\n",
      "|          Nov 01|        370|\n",
      "|          Nov 06|        357|\n",
      "|          Dec 10|        979|\n",
      "|          Nov 04|        355|\n",
      "|          Dec 17|        221|\n",
      "|          Nov 12|        720|\n",
      "|          Dec 03|       1197|\n",
      "|          Nov 11|        149|\n",
      "|          Nov 10|         93|\n",
      "|          Nov 30|        278|\n",
      "|          Dec 04|        884|\n",
      "|          Nov 29|        952|\n",
      "|          Nov 05|       1058|\n",
      "|          Nov 26|        521|\n",
      "|          Nov 19|        457|\n",
      "|          Dec 07|        360|\n",
      "|          Nov 13|        226|\n",
      "|          Dec 12|        372|\n",
      "|          Dec 16|        363|\n",
      "+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Counting the number of tweets in each date\n",
    "tweetcount = data_pyspark3.groupby(data_pyspark3.tweet_created_at).agg(sq.count('id_str').alias('tweet_count'))\n",
    "tweetcount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|tweet_created_at|tweet_count|\n",
      "+----------------+-----------+\n",
      "|          Jan 03|       1536|\n",
      "|          Jan 10|       1508|\n",
      "|          Jan 11|       1496|\n",
      "|          Jan 12|       1410|\n",
      "|          Jan 06|       1364|\n",
      "|          Jan 07|       1360|\n",
      "|          Jan 20|       1336|\n",
      "|          Mar 02|       1296|\n",
      "|          Jan 13|       1295|\n",
      "|          Jan 14|       1290|\n",
      "|          Jan 21|       1290|\n",
      "|          Jan 18|       1286|\n",
      "|          Dec 15|       1276|\n",
      "|          Jan 24|       1259|\n",
      "|          Nov 18|       1246|\n",
      "|          Dec 03|       1197|\n",
      "|          Jan 02|       1195|\n",
      "|          Jun 27|       1191|\n",
      "|          Jul 04|       1190|\n",
      "|          Jan 19|       1175|\n",
      "+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SQLContext\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "#Finding the date with the highest number of tweets\n",
    "tweetcount.registerTempTable('tweetcount')\n",
    "tweetcount = sqlcontext.sql('SELECT * FROM tweetcount order by tweet_count desc')\n",
    "tweetcount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               text_|total|\n",
      "+--------------------+-----+\n",
      "|@amazon worst sho...|    5|\n",
      "|@ItsJosshA We alw...|    3|\n",
      "|@ItsJosshA Oh no!...|    2|\n",
      "|@KStefl Sounds li...|    2|\n",
      "|@Schoey1992 Happy...|    2|\n",
      "|@ratbones666 You ...|    2|\n",
      "|@ThorpPerrow Awww...|    2|\n",
      "|@thedexterouz Hi!...|    2|\n",
      "|@matt_linsley Ple...|    1|\n",
      "|@VlSlT Sorry to h...|    1|\n",
      "|@PPramod2041984 H...|    1|\n",
      "|@mailstosandeep H...|    1|\n",
      "|@Elidan_8 Here's ...|    1|\n",
      "|@joyfulneesh Than...|    1|\n",
      "|@brooklynnnross I...|    1|\n",
      "|@DurhamBelle I'm ...|    1|\n",
      "|@heypardeep We're...|    1|\n",
      "|@__NaijaDrew Sorr...|    1|\n",
      "|@magsophazjon Ple...|    1|\n",
      "|@magsophazjon Hey...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SQLContext\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "#Calculating the sum of “favorite_count” and “retweet_count” for each tweet on that day. Then reporting the text content (“text_”) of the top 100 tweets with highest sum. \n",
    "data_pyspark3.registerTempTable('tempdata')\n",
    "retweets_sum = sqlcontext.sql(\"SELECT text_,favorite_count+ retweet_count as total from tempdata where tweet_created_at = 'Jan 03' order by total desc limit 100\")\n",
    "retweets_sum.show()\n",
    "\n",
    "text1 = retweets_sum.toPandas()\n",
    "a = text1['text_'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewkondrak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matthewkondrak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#DataCleaning: Removing stopwords, removing punctuation\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def cleaning(textfile):\n",
    "    y = textfile\n",
    "    cleaned1=[]\n",
    "    \n",
    "    for i in y:\n",
    "      i.replace(\"'\", \" \")\n",
    "      cleaned1.append(i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))))                 \n",
    "    \n",
    "    output1=[]\n",
    "\n",
    "    for i in cleaned1:\n",
    "        output1.append(\" \".join([w.lower() for w in i.split() if w.isalpha()]))    \n",
    "    return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  frequency\n",
      "356  the         65\n",
      "355   to         65\n",
      "75   you         64\n",
      "223    t         61\n",
      "214  for         50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#calculating the word frequency of the 100 tweets\n",
    "cleaned1 = cleaning(a)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "w = cleaned1\n",
    "tuple(w)\n",
    "spark_rdd = sc.parallelize(w)\n",
    "word_frequency = spark_rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "\n",
    "#saving to csv\n",
    "goal1=pd.DataFrame(word_frequency)\n",
    "goal1.columns = ['word','frequency']\n",
    "print(goal1.sort_values(by=['frequency'], ascending=False).head(5))\n",
    "goal1.to_csv(r\"/Users/matthewkondrak/desktop/goal1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal 2:\n",
    "\n",
    "Using find_text.csv. The csv consists of two columns: \"id_str\" and \"text\" with the \"text\" column empty\n",
    "\n",
    "Finding out the text content of each tweet according to “id_str” while joining Amazon_Tweets.csv and filling in the “text” column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|              id_str|text|\n",
      "+--------------------+----+\n",
      "|'793270689780203520'|null|\n",
      "|'793281386912354304'|null|\n",
      "|'793299404975247360'|null|\n",
      "|'793301295255945216'|null|\n",
      "|'793315815411978240'|null|\n",
      "|'793322306848292864'|null|\n",
      "|'793322433625415680'|null|\n",
      "|'793365409047023616'|null|\n",
      "|'793369654878232577'|null|\n",
      "|'793375905280393216'|null|\n",
      "|'793376242837823488'|null|\n",
      "|'793378044052406272'|null|\n",
      "|'793378188416131072'|null|\n",
      "|'793379112685568000'|null|\n",
      "|'793381418395136000'|null|\n",
      "|'793382930085253121'|null|\n",
      "|'793383832720474113'|null|\n",
      "|'793386133434593280'|null|\n",
      "|'793386974459682816'|null|\n",
      "|'793390636619759616'|null|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing find_text.csv\n",
    "goal2 = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/Users/matthewkondrak/Desktop/find_text.csv\")\n",
    "goal2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              id_str|               text_|\n",
      "+--------------------+--------------------+\n",
      "|'793382930085253121'|@mybharatraj Hi! ...|\n",
      "|'793441656984903680'|@AmazonHelp done,...|\n",
      "|'793517259880861696'|Your customer ser...|\n",
      "|'793533066157387776'|@flamablebrownie ...|\n",
      "|'793542659625349121'|So now @AmazonHel...|\n",
      "|'793579129333510144'|@jpokeefe That's ...|\n",
      "|'793750855300313088'|@Rajleom10 Hi the...|\n",
      "|'793813919106228224'|@AmazonHelp I ord...|\n",
      "|'795621294226231300'|@AmazonHelp is my...|\n",
      "|'793839943386791936'|@AmazonHelp Just ...|\n",
      "|'793874173919649794'|@rava_diganta Hey...|\n",
      "|'793843252810088448'|60 rs product in ...|\n",
      "|'793848101975056388'|I *never* had any...|\n",
      "|'794246147212767232'|@RUSSON82 We'd li...|\n",
      "|'793901849552322560'|@AmazonHelp tryin...|\n",
      "|'794376470659551232'|@imyashpal Hi the...|\n",
      "|'794259803744960521'|@AmazonHelp that ...|\n",
      "|'794253312895942657'|@Not_Domo We're a...|\n",
      "|'794291925926952960'|I feel like I've ...|\n",
      "|'794321892291334145'|@Pizza711 Was the...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#creating now table\n",
    "goal2.registerTempTable('goal2')\n",
    "data_pyspark.registerTempTable('tweet')\n",
    "\n",
    "#joining\n",
    "output2 = sqlcontext.sql(\"SELECT DISTINCT I.id_str,T.text_ from goal2 I JOIN tweet T on I.id_str = T.id_str\")\n",
    "output2.show()\n",
    "\n",
    "#saving output\n",
    "output3 = output2.toPandas()\n",
    "output3.to_csv(r\"/Users/matthewkondrak/Desktop/goal2.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
